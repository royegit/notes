
QPS 每秒查询量 &TPS 代表每秒执行的事务数量
10ms(毫秒) 处理一个sql
1s(秒) 	处理100个sql
qps <=100 



达标带来的问题
	

影响数据库的因素：
	sql查询速度，服务器硬件，网卡流量，磁盘IO
超高的QPS和TPS
	风险：效率低下的sql 访问量


大量的并发和超高的CPU使用率
	风险：
		大量的并发：
			数据库链接数被占满（max_connections默认100）

	超高的cpu使用率：
		因CPU资源耗尽而出现宕机

磁盘IO：
	风险：
		磁盘IO性能突然下降（使用更快的磁盘设备）
		其他大量消耗磁盘性能的计划任务（调整计划任务）
网卡流量
	风险：网卡IO被占满（1000Mb/8≈100MB）

	如何避免无法链接数据库的情况：
	1、减少从服务器的数量（因为每一从服务器的数量都要从主服务器上去复制日志所以从服务器越多网络流量就越大）
	2、进行分级缓存（一定要避免前端大量的缓存突然失效，对数据库流量的冲击）
	3、避免使用 "select *" 进行查询
	4、分离业务网络和服务器网络（这样就可以避免由于主从同步或是网络备份这样的作业的影响，影响网络性能）

大表给我们带来的问题
	什么样的表称之为大表呢？
		1、记录行数巨大、单表超过千万行
		当然这都是相对的要结合我们的业务场景和磁盘IO情况而定
		如果我们这个表只是用来记录日志的，也就是说只要insert操作和少量的select操作而几乎没有updata和delete操作的化这样的表就算是操过了一千万行也不会对我们的数据造成什么样的影响
		但是也是有例外的情况比如我们以前就遇到过一张这样的表，表中的记录已经差不多10个亿了但是工作一直很稳定因为很少有人去关注他只是用于记录一些操作日志。
		但是当有一天业务发生了变更我们要对这个表加入更多的列加入更多的内容的时候，这时会发现要给这样的表增加列就会是一件十分痛苦的事情特别是当这张表还被同步到N给服务器上的时候那就可以说是灾难了
		
		2、表数据文件巨大、表数据文件超过10G
		例外一个大表的定义是数据库文件超过10G这是对于普通的磁盘来说如果我们使用的是csaIO这样的高速磁盘的话这个线程可能就会大一些那我们现在就来看看大表给我们带来什么样的问题。
	
	大表对查询的影响
		慢查询：很难在一定时间内过滤出所需的数据
			订单来源日志表在这个表中记录了所以订单的来源日志（来源少）->区分度低->所以从上亿的表中去查询一部分数据就会使用大量的磁盘IO->同时效率也会很低 
			如果扩大这个问题假设我们需要在页面中显示一个商品从不同渠道来源的订单以促进用户的购买。那这个查询就会每次用户访问商品时都会被执行，这样就会产生大量的慢查询从而严重拖慢了网站的访问
			这所以这样说是因为这就是我所遇到过的一个案例（当然这种sql都是有办法优化的在后面会讲到）
		大表对DDL操作的影响
			首先来说假如我们想在大表中想建立索引这个时候就需要很长的时间
				风险：
					MySQL版本<5.5建立索引会锁表（当在mysql在5.5之前的版本中，如果我们使用的是mysql自带的这种InnoDB的话对于建立索引的操作实际上是会锁表的）
					MySQL>=5.5 虽然不会锁表但会引起主从延迟
			修改表结果需要长时间锁表
				风险：
					会造成长时间的主从延迟
						这就和建立索引一样了由于我们目前MySQL主从复制的这种机制对于所有的这种DDL操作全是现在主库上完成之后在通过日志传输到从库上然后在再从库上执行相同的操作这样来完成这种表结构变成复制的，
						假设我们在一张表在主库上进行修改的时候需要480秒的时间来完成一个表结构的修改那在从服务器上同样至少也要480秒来完成这个修改因为目前现在mysql的主从复制呢都是使用单线程所以一旦有了大表的修改从服务器上没有完成相关修改操作之前呢
						其他的所有数据库修改操作都会无法执行这样就找出了只是480秒的出从延迟这对于大多数用来说都是无法接受的虽然的mysql5.6版本中已经支持了多线程复制但是也会有一个限制这个我们在后面在说
					影响正常的数据操作
						在这种主库上进行表结构修改的时候这个表的所有操作呢可能就会被组赛这样就会影响数据库的这种正常的操作


如何处理数据库中的大表 
	分库分表把一张大表分成多个小表
		难点：
			分表主键的选择（根据业务的不同可以有多种分表的方式）
			分表后跨分区数据的查询和统计
				（分库分表的优化，为了解决订单表和订单明细表数据量过大的这种问题公司决定把订单相关表进行分库分表操作当时是调用了大量的人力和物力来进行这个项目最终耗时将近两个月终于完成了
				  并在下一次公司大促销前上线虽然说效果还是不错但是其中也出现了很多问题特别是分库分表之后为了继续使用原有的订单相关的统计报表以及后台的操作功能就不得不通过一些手段把已经分
				  开的一些表合并到一台服务器上作为后端统计和管理所使用的这种数据库。
				  但是分库分表操作需要消耗大量的人力和物力同还有帽子后端影响业务的这种风险所以我个人认为分库分表并不适合所有公司进行其实在大多数情况下我们可以选择另外的一种方式来对大表进行处理
				另外一种大表的处理就是“大表的历史数据进行归档”
					
				 ）
			大表的历史数据归档
				而特别是对那种日志类型的大表呢甚至是像订单表这样的业务表我们也可以进行历史数据归档进行历史数据归档的好处就是可以尽量的减少对前后端业务的影响因为对于前端业务来说呢实际上表结构并没有任何变化所以一切的程序都会正常的使用
				而只是对于订单表这样的业务表如果我们要进行归档则需要在前端多增加一个历史查询的入口但是可以说已经归档的历史是很少有人去查询的所以就算归档表很大对正常业务的影响也是很轻微的而且我们的归档表还可以现在正在使用的这个表放在
				不同的服务器上这样呢一方面减少了热数据所在服务器表容量同时也减少了核心服务器的这种查询压力而对于后端业务来说呢已经归档的数据应该是已经完成相关操作和统计的历史数据所以呢对于后端的应用也不会有什么影响但是为了完成上面的
				所说的不影响前后端业务的现有业务逻辑呢我们在进行历史数据归档时同样面临着两个难点
				难点：
					其一、
						归档时间点的选择	
							前面说到了归档的这种数据应该是已经不会被使用或者是很少使用的数据所以这个归档的时间点就要很认真的来选择
							比如，订单列表我们可以选择归档一年前的数据也可以选择归档一个月前的数据那从这里看对于订单列表来说归档一
							年前的数据可能就更为合适而对于纯日志的数据呢我们可以归档前一个周的数据可能就可以了去年归档操作的
					其二、
						如何进行归档操作
							前面说了对于大的增删改查操作 都要十分的小心如果我们要进行归档就要把已经归档的历史数据从这个线有的表中进
							行删除那么如果我们对一个上亿行的表一下子删除几十万行或者上百万行的数据那么就一定要讲究一些操作的方式了
							不然会造成轻的这个主从的延迟重则会产生大量的组赛从而影响我们的正常的业务访问对于如和对大表进行增删改查



			
大事务给我们带来的影响

	1、事务是数据库系统区别于其他一切文件系统的重要特性之一
		对于文件系统中如果要保证两个文件一致而在我们修改完一个文件后系统突然崩溃了这样文件呢系统在恢复后很难保证两个两个文件的一致。
		而数据库系统中呢由于使用了事务所以通常在数据库服务器突然间崩溃后我们可以恢复数据库中的数据使数据还是保持一致性。
	2、事务是一组具有原子性的SQL语句，或是一个独立的工作单元
		这一组SQL呢可以说一组简单的查，询也可以是一组由多个增删改查的SQL所组成的一组SQL语句的集合但是这一组SQL呢是具有原子性的也就是说
		事务中的所有SQL要么全部完成要么全部失败
		
	从以上两个描述呢我们就可以看出事务是要从以下特性的
		
		
		事务：
			原子性，
				一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功要么全部失败，对于一个事务来说，不可能
				只执行其中的一部分操作
				（银行的例子：现在的银行卡中常常是有多个账户的比如大多数会有一个理财账户一个活期存取账户如果我们想从理财账户中转出两千元到
				  银行存款账户通常我们要进行以下几步操作
					1、检查理财账户中的余额是否高于2000元
					2、从理财账户的余额中减去2000元
					3、在活期存款账户上增加2000元
					（以上3给步骤必须作为一个整体一起完成）
					（如果进行到第二步系统就崩溃了如果没有事务的原子性会发生什么呢毫无疑问那样的话我们就会损失2000元）
					（而如果是以上三个步骤在一个事务中完成在当执行到第二步时系统崩溃了在系统恢复后就会发现在日志中有没有提交的事务，这时就
					  会已经执行的第二步操作进行回滚这样就避免了用户的损失）
					  （所有整个事务中的所有操作要么全部提交成功，要么全部失败回滚）
				）
			一致性，
				定义
					一致性是指事务将数据库从一种一致性状态转换到另外一种一致性状态，在事务开始之前和事务结束后数据库中数据的完整性没有被破坏
					（在事务的开始之前和事务的结束之后数据库中的完整性不应该被破坏概念呢总是让人难以理解）
					（上面的银行的例子：就是在转账之前和转账之后我们的账户的总金额不应该有任何的变化，转账之前在我们的理财账户中有2000元余额
					  而转账之后呢这两千元余额呢过度到了我们的活期存款账户中但是呢账户中的总余额数是不变的从始至终都是2000元。
					  这是看似很简单的事情了，但是在一些不支持事务的系统中是很难做到的
					）
			隔离性，
				隔离性要求一个事务对数据库中数据的修改，在未提交完成前对其他事务是不可见的。
				（同样是上面的银行的例子：当我们在执行完第二步在理财账户中减去2000元之后如果这时候另外一个事务需要对所有银行理财账户中的余额
				  进行汇总这时这些事务应该还能够在我们的理财账户中看到我们已经转出的那2000元，这是由于我们第一给事务减去两千块钱但是呢事务还
				  没有提交所以对于第二个事务来说应该是不可见的这就是事务的隔离性。 
				 ）
				 SQL标准中定义的四种隔离级别
					未提交读（READ UNCOMMITED）：
						所谓的未提交读就是在未提交读这种隔离级别中，事务中对数据的进行修改即使事务还没有被提交对于其他事务也都是可见的。
						事务可以读取未提交的数据，也就是称之为“脏读”。对提交的数据呢我们通常也称之为脏数据
						（脏数据：通俗的讲，当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，
						然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是脏数据，依据脏数据所做的操作可能是不正确的）
						（之前在使用SQLserver的时候DBA经常会建议在进行查询时需要在from子句中加入 wait onglong 加入这个关键字。这个关键字的作用就是要进脏读
							最主要的就是在老版本的SQLserver中是不支持那种非锁定组合的所以也是一种没有办法的事情 当然在新的SQLserver中已经有办法进行非锁定读了
							所以通常情况下是不建议大家在去使用脏读这个隔离级别的因为这个可能会造成很多数据逻辑上的问题那第二个事务的隔离级别
						）
					已提交读（ READ COMMITTED ）：
						（相信大家对已提交读已经很熟悉了因为这是大多数数据库系统中的默认的隔离级别比如说Oracle，SQLserver，PostgreSQL它们默认的隔离级别都
						  是提交读但是MySQL是给例外已提交读满足前面所提到的隔离性级别的简单定义也就是说一个事务开始时只能看到已提交的事务所做的修改或者说一个
						  事务从开始到提交前对数据所做的修改对于其他的事务来说是不可见的这就是已提交读。
						 ）
					可重复读（ REPEATABLE READ）:
						 该级别保证了在同一个事务中多次读取同样的记录的结果是一致的，相比于可重复读这种隔离级别呢已提交读又可称之为不可重复读 。那这俩个级别到底
						 有什么区别呢？
						 （例子：
							表 create  table if not exists  `test`(`id` INT UNSIGNED AUTO_INCREMENT ,PRIMARY KEY(`id`))engine=innoDB DEFAULT CHARSET=utf8;
								insert into test(`id`) values(1);
								insert into test(`id`) values(3);
								insert into test(`id`) values(5);
								insert into test(`id`) values(7);
								insert into test(`id`) values(9);
								insert into test(`id`) values(10);
						  ）
						  
						  --------------------------------------------------------------------------------------------------------
						  mysql> select *from test;
						  +----+
						  | id |
						  +----+
						  |  1 |
						  |  3 |
						  |  5 |
						  |  7 |
						  |  9 |
						  | 10 |
						  +----+
						  6 rows in set (0.00 sec)
						  mysql> show variables like '%iso%';  #查看隔离级别
						  +---------------+------------------+
						  | Variable_name | Value            |
						  +---------------+------------------+
						  | tx_isolation  | REPEATABLE -READ | #可重复读
						  +---------------+------------------+
						  1 row in set (0.01 sec)
						  mysql> begin;						   #启动事务
						  Query OK, 0 rows affected (0.00 sec)

						  mysql> select *from test where id<7;
						  +----+
						  | id |
						  +----+
						  |  1 |
						  |  3 |
						  |  5 |
						  +----+
						  3 rows in set (0.00 sec)
						  
						  mysql> 
						--------------------------------------------------------------------------------------------------------	
						
					打开第二个进程（第二个连接）进入数据库
						--------------------------------------------------------------------------------------------------------
						  Connecting to 192.168.0.106:22...
						  Connection established.
						  To escape to local shell, press 'Ctrl+Alt+]'.
						  
						  Last login: Tue Jul  3 11:36:06 2018 from 192.168.0.104
						  [root@localhost ~]# mysql -uroot -p
						  Enter password: 
						  Welcome to the MySQL monitor.  Commands end with ; or \g.
						  Your MySQL connection id is 56
						  Server version: 5.5.60-log Source distribution
						  
						  Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
						  
						  Oracle is a registered trademark of Oracle Corporation and/or its
						  affiliates. Other names may be trademarks of their respective
						  owners.
						  
						  Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
						  
						  mysql> use test;
						  Database changed
						  mysql> show tables;
						  +----------------+
						  | Tables_in_test |
						  +----------------+
						  | test           |
						  | users          |
						  +----------------+
						  2 rows in set (0.00 sec)
						  
						  mysql> begin;							#启动事务
						  Query OK, 0 rows affected (0.00 sec)
						  
						  mysql> insert into test value(2);		#插入一个实体数据
						  Query OK, 1 row affected (0.00 sec)
						  
						  mysql> commit;						#提交数据
						  Query OK, 0 rows affected (0.06 sec)
						  
						  mysql> 
						  
						--------------------------------------------------------------------------------------------------------
						
						在回到第一个连接做同样的搜索
						--------------------------------------------------------------------------------------------------------
						  mysql> select *from test where id<7;
						  +----+
						  | id |
						  +----+
						  |  1 |
						  |  3 |
						  |  5 |
						  +----+
						  3 rows in set (0.00 sec)
						  
						  mysql>
						--------------------------------------------------------------------------------------------------------
						发现还是1、3、5并看不到在第二个连接中已经提交的数字2
			
			
						那现在我们如果我们把当前的事务隔离级别改一下（第一个连接）
						--------------------------------------------------------------------------------------------------------
						mysql> commit;						#先终止事务
						Query OK, 0 rows affected (0.00 sec)
						
						mysql> set session tx_isolation='read-committed';  #设置事务隔离级别（设置成读已提交）
						Query OK, 0 rows affected (0.00 sec)

						mysql> show variables like '%iso%';				   #查看事务级别是否修改成功
						+---------------+----------------+
						| Variable_name | Value          |
						+---------------+----------------+
						| tx_isolation  | READ-COMMITTED |
						+---------------+----------------+
						1 row in set (0.00 sec)
						
						mysql> begin;									   #启动事务
						Query OK, 0 rows affected (0.00 sec)
						
						mysql> select *from test where id<7;			   #开始刚才的搜索，发现已经读到刚才插入的2了（因为已经重启事务了看到2是正常的所以需要在第二个链接中在插入一条数据以确保第一个链接在这个事务中是否真的可以读取到数据）
						+----+
						| id |
						+----+
						|  1 |
						|  2 |
						|  3 |
						|  5 |
						+----+
						4 rows in set (0.00 sec)

						--------------------------------------------------------------------------------------------------------
						
						第二个链接
						--------------------------------------------------------------------------------------------------------
						mysql> insert into test values(4);  #这回插入一个4
						Query OK, 1 row affected (0.04 sec)

						mysql> commit;						#提交事务
						Query OK, 0 rows affected (0.00 sec)
						
						--------------------------------------------------------------------------------------------------------
						
						
						回到第一个链接；同样查询id小于7的数字
						--------------------------------------------------------------------------------------------------------
						
						mysql> select *from test where id<7;
						+----+
						| id |
						+----+
						|  1 |
						|  2 |
						|  3 |
						|  4 |
						|  5 |
						+----+
						5 rows in set (0.00 sec)
						
						mysql> 

						--------------------------------------------------------------------------------------------------------
							我们可以看到id为4的这个数字了，但是这个事务我们还没有被提交所以说在这种读已提交的情况下我们是不可重复读的
						因为我们在同一事务中我们两次执行相同的SQL得到的结果是不一样的，而在可重复读的这个实验中虽然说接二连三重
						复同样提交了一个事务但是我们多次执行同一个SQL的结果是一致的这就是称之为可重复读，这就是可从复读和已提交
						读之间的这种差别
			
					可串行化（SERIALIZABLE）
						串行化是最高的隔离级别。简单来说呢串行化会对读取的每一行数据上都加锁所以可能会导致大量的锁超时和锁死的问题
						所以在实际应用中我们很少应用到这个隔离级别除非是严格要求数据一致性并且可以结束在没有并发的前提下我们才会考
						虑用这种隔离级别
						
						这上面这四种隔离级别从隔离性来说从低到高一次是 未提交读->已提交读->可重复读->可串行化读。
						而从并发性来说从低到高一次是 				  可串行化读->可重复读->已提交读->未提交读。
						
						对于innoDB来说他默认的隔离级别是 “可重复读”
			
			
			持久性：
				定义：
					一旦事务提交，则其所有做的修改都会永久保存到数据库中。此时即使系统崩溃，已经提交的修改数据也不会丢失。
					这里所说的持久性只是相对来说的，只能从数据库的角度来说保证事务的持久性而不包括一些外部因素如磁盘损坏
					这种情况而导致的数据丢失，就不是这里所说的事务持久性可解决的问题了，要真正保证在磁盘损坏的情况下也不
					丢失数据的话就只有靠数据的备份或者是异地的复制这样的高可用架构才能够做到。	
			
			
		大事务：
			定义：运行时间比较长，操作的数据比较多的事务
				大家都接触过余额宝这样的理财产品这种理财产品的特点就是每天都会计算用户的前一天的理财收入的所得而如果我们在一个事务中
				对所有的用户的理财收入进行计算并更新到用户的余额中这是数以亿计的用户余额的更新可能就需要数个小时而且一旦中间出现任何
				的问题会进行回滚事物所需要的时间可能就更加的不可估量，更不要说的更新过程中对所有的相关账户都会加锁造成用户无法使用余
				额这样的问题像这样的事物就称之为“大事务”。
				当余额宝的收入计算和更新并不是在一个事务中完成，所以我们也没有遇到过像上面说的这种情况但是从上面的这个例子中我们不难
				
				看出大事物对性能的影响主要有以下几个：
					风险：	
						锁定太多的数据，造成大量的组赛和锁超时。
						对于InnoDB这种事务型存储引擎来说虽然使用的是行级锁，但是在一个事务中为了保证事务的这种一致性通常会把事务中所
						有处理的相关的这种都会加锁。
						如果我们所涉及到的数据比较多比如上面的例子中，几乎会涉及到所有用户记录。那这样的结果就会把所有的记录全部锁住，
						这时如果有用户需要用余额宝支付就会产生组赛在并发比较大的情况下会直接使用数据库的服务器呢被大量的链接所占满同
						时会严重影响数据库的性能和稳定性。而大事物给我们带来的第二个风险就是。
						
						回滚所需要的时间比较长并且在回滚的过程中所有的被锁定的数据仍然被锁定。那回滚所需要的的时间可能比我们所进行的
						事务化的时间还要长。
						大事务给我们带来的第三个风险就是容易造成主从延迟从mysql主从复制的特点来看只有在主库上事务执行完之后并把日志写
						入到binlog中这时从服务器才能通过binlog进行同步。大家可以想一下如果主上的事务执行了几个小时后在提交并写入binlog
						中那是不是说明主从之间会延迟几个小时呢。以上就是大事物给我们带来的几个性能问题，当然实际上还远远不只这些。只
						不过这几个风险是比较重要的而已那我们要怎样处理大事物呢？

				如何处理大事物:
					1、避免一次性处理太多数据
						对于一次需要操作几百上千万数据的这种操作我们可以采用分批次处理的这种方法这里所说的操作是指增删改查这样的操作
						比如上面的例子中每天处理的用户理财事务的处理，我们可以分成一万用户一批处理完一批提交之后在处理另一批这样就大
						大减少了我们事务的大小避免了上面所说的种种问题
					
					2、移出不必要在事务中的select操作	
						要把不必要的select操作从事务中移出，在平时的工作中事务执行比较长的情况多数是在事务中使用了大量的查询这种操作
						而这些查询操作本身是可以不在事务中完成的对于这种情况，就需要把要查询的从事务中移出去保证事务中只有必要的写操
						作如果可以做到以上两点我们基本上可以说就避免了大事物的产生了





























































































































